{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tweet-sentiment with Random Forest\n",
    "\n",
    "In this notebook, we analyse the tweet-sentiment dataset using a random forest classifier. Random forest models are very popular as they tend to be very robust in predicting and are resistant to overfitting to the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set-up\n",
    "\n",
    "We begin by loading the necessary libraries as well as our own `eda` module, which consists of the functions used in exploratory analysis of the tweet-sentiment dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eda\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility purposes, we also set a seed value of 42 throughout the entire project: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "\n",
    "Using the functions defined in our eda module, we load and do simple cleaning of the dataset. This includes: \n",
    "- removing NA values; \n",
    "- converting the categorical variable `sentiment` to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eda.load_dataset(url=\"https://raw.github.com/ant1code/tweet-sentiment/master/data/train.csv\")\n",
    "\n",
    "df = eda.clean_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset consists of 27485 tweets.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spent the entire morning in a meeting w/ a ven...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh! Good idea about putting them on ice cream</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>says good (or should i say bad?) afternoon!  h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont think you can vote anymore! i tried</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha better drunken tweeting you mean?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  Spent the entire morning in a meeting w/ a ven...          1\n",
       "1      Oh! Good idea about putting them on ice cream          2\n",
       "2  says good (or should i say bad?) afternoon!  h...          1\n",
       "3         i dont think you can vote anymore! i tried          0\n",
       "4             haha better drunken tweeting you mean?          2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"This dataset consists of {len(df)} tweets.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing\n",
    "\n",
    "Similar to in our EDA notebook, we normalise the data by removing stopwords and lemmatizing the remaining words. This reduces the number of words to be processed, thereby reducing the complexity of our eventual random forest model and making the training process more efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spent the entire morning in a meeting w/ a ven...</td>\n",
       "      <td>1</td>\n",
       "      <td>spent entire morning meeting w vendor boss not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh! Good idea about putting them on ice cream</td>\n",
       "      <td>2</td>\n",
       "      <td>oh good idea put ice cream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>says good (or should i say bad?) afternoon!  h...</td>\n",
       "      <td>1</td>\n",
       "      <td>say good say bad afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont think you can vote anymore! i tried</td>\n",
       "      <td>0</td>\n",
       "      <td>dont think vote anymore try</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha better drunken tweeting you mean?</td>\n",
       "      <td>2</td>\n",
       "      <td>haha well drunken tweet mean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  Spent the entire morning in a meeting w/ a ven...          1   \n",
       "1      Oh! Good idea about putting them on ice cream          2   \n",
       "2  says good (or should i say bad?) afternoon!  h...          1   \n",
       "3         i dont think you can vote anymore! i tried          0   \n",
       "4             haha better drunken tweeting you mean?          2   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  spent entire morning meeting w vendor boss not...  \n",
       "1                         oh good idea put ice cream  \n",
       "2                         say good say bad afternoon  \n",
       "3                        dont think vote anymore try  \n",
       "4                       haha well drunken tweet mean  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = eda.get_stop_words()\n",
    "df = eda.normalize(df, stop_words)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to *vectorize* the lemmatized tweets. This refers to the process of converting text (string features) into vectors (of numerical features) that can be understood by the random forest classifier. \n",
    "\n",
    "We chose to use sklearn's TfidfVectorizer for this purpose. TFIDF stands for *term frequency-inverse document frequency* and in this project, it is a statistic that reflects how important a word is to a tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21359 features.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "bow = vectorizer.fit_transform(df['lemmatized'])\n",
    "\n",
    "print(f\"There are {len(vectorizer.get_feature_names())} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results in a very large and very sparse term frequency matrix, since there are certainly many words that appear only very few times in the entire dataset. A lot of space is wasted in representing such matrices. To get around this problem, we specify a *cut-off* of 5: it means that when the vectorizer builds the vocabulatry, it should ignore terms that appear in fewer than five tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3749 features.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "bow = vectorizer.fit_transform(df['lemmatized'])\n",
    "\n",
    "print(f\"There are {len(vectorizer.get_feature_names())} features.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Random Forest\n",
    "\n",
    "We are ready to proceed with training our classifier! The first step is to split the dataset into train and test sets to avoid contamination of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(bow, sentiment, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialise the random forest classifier and fit it to the train set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(random_state=seed, bootstrap=False)\n",
    "classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Random Forest \n",
    "\n",
    "It remains to evaluate the performance of our random forest classifier on the test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6978282438540403"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus: Hyperparameter Optimization \n",
    "\n",
    "The score above is decent (and in fact, better than our human predictions), but it is not nearly as high as the scores obtained by the DistilBERT and BERT classifiers. To improve the performance of the random forest classifier, we can search for optimal hyperparameters like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:  3.5min remaining:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  4.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs...\n",
       "                                        'max_features': ['sqrt'],\n",
       "                                        'min_samples_leaf': [16, 32],\n",
       "                                        'min_samples_split': [320, 640, 1280,\n",
       "                                                              2560],\n",
       "                                        'n_estimators': [2000, 2010, 2020, 2030,\n",
       "                                                         2040, 2050, 2060, 2070,\n",
       "                                                         2080, 2090, 2100, 2110,\n",
       "                                                         2120, 2130, 2140, 2150,\n",
       "                                                         2160, 2170, 2180, 2190,\n",
       "                                                         2201, 2211, 2221, 2231,\n",
       "                                                         2241, 2251, 2261, 2271,\n",
       "                                                         2281, 2291, ...]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=True, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BONUS: hyperparameter optimization\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 2000, stop = 4000, num = 200)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(100, 2000, num = 100)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [320, 640, 1280, 2560]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [16, 32]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator = classifier, param_distributions = random_grid,\n",
    "                                   cv = 3, verbose = 2, random_state = seed, \n",
    "                                   n_iter=3, n_jobs= -1, return_train_score = True)\n",
    "\n",
    "random_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 3587,\n",
       " 'min_samples_split': 640,\n",
       " 'min_samples_leaf': 16,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 1558,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the best of the 6 trained models\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the random forest with these hyperparameters performs better than our initial random forest classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=1558, max_features='sqrt',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=16, min_samples_split=640,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=3587,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_classifier = RandomForestClassifier(**random_search.best_params_)\n",
    "new_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6832763752618234"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_classifier.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
